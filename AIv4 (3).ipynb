{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "shML4tx0fdJ4",
        "outputId": "e293e164-7537-4c3d-895a-348a119adf69"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OBXAQIlOg-tk",
        "outputId": "33c9baa6-4fee-4700-95e1-99f1cfc0d1b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "CHECKING FOR PRETRAINED MODELS\n",
            "======================================================================\n",
            "\n",
            "âœ“ ALL 3 PRETRAINED MODELS FOUND!\n",
            "âœ“ Models are in Google Drive\n",
            "\n",
            "OPTION 1 (FAST - Skip training):\n",
            "  Run the 'LOAD PRETRAINED' cell below\n",
            "  Results in 5 seconds!\n",
            "\n",
            "OPTION 2 (Full - Retrain everything):\n",
            "  Run all cells normally\n",
            "  Training takes 2 hours\n",
            "\n",
            "SKIP_TRAINING = True\n"
          ]
        }
      ],
      "source": [
        "# ==================== CELL 0: CHECK FOR PRETRAINED MODELS ====================\n",
        "import os\n",
        "\n",
        "models_dir = '/content/drive/MyDrive/CSE311_Final_Project'\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"CHECKING FOR PRETRAINED MODELS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "if os.path.exists(models_dir):\n",
        "    model_files = [f for f in os.listdir(models_dir) if f.endswith('.pt')]\n",
        "    figure_files = [f for f in os.listdir(models_dir) if f.endswith('.png')]\n",
        "\n",
        "    if len(model_files) == 3:\n",
        "        print(\"\\nâœ“ ALL 3 PRETRAINED MODELS FOUND!\")\n",
        "        print(\"âœ“ Models are in Google Drive\")\n",
        "        print(\"\\nOPTION 1 (FAST - Skip training):\")\n",
        "        print(\"  Run the 'LOAD PRETRAINED' cell below\")\n",
        "        print(\"  Results in 5 seconds!\")\n",
        "        print(\"\\nOPTION 2 (Full - Retrain everything):\")\n",
        "        print(\"  Run all cells normally\")\n",
        "        print(\"  Training takes 2 hours\")\n",
        "        SKIP_TRAINING = True\n",
        "    else:\n",
        "        print(f\"\\nâš ï¸ Only {len(model_files)}/3 models found\")\n",
        "        print(\"Will train from scratch...\")\n",
        "        SKIP_TRAINING = False\n",
        "else:\n",
        "    print(\"\\nğŸ“ Project folder doesn't exist yet\")\n",
        "    print(\"Will create and train from scratch...\")\n",
        "    SKIP_TRAINING = False\n",
        "\n",
        "print(f\"\\nSKIP_TRAINING = {SKIP_TRAINING}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "6pqlYzUbiIry",
        "outputId": "a1f72c26-47dd-4797-942b-06e9fa835cae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting seaborn\n",
            "  Downloading seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting statsmodels\n",
            "  Downloading statsmodels-0.14.5-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (9.5 kB)\n",
            "Collecting patsy\n",
            "  Downloading patsy-1.0.2-py2.py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting numpy!=1.24.0,>=1.20 (from seaborn)\n",
            "  Downloading numpy-2.3.4-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (62 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.1/62.1 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pandas>=1.2 (from seaborn)\n",
            "  Downloading pandas-2.3.3-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (91 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m91.2/91.2 kB\u001b[0m \u001b[31m231.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting matplotlib!=3.6.1,>=3.4 (from seaborn)\n",
            "  Downloading matplotlib-3.10.7-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\n",
            "Collecting scipy!=1.9.2,>=1.8 (from statsmodels)\n",
            "  Downloading scipy-1.16.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (62 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m204.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting packaging>=21.3 (from statsmodels)\n",
            "  Downloading packaging-25.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting contourpy>=1.0.1 (from matplotlib!=3.6.1,>=3.4->seaborn)\n",
            "  Downloading contourpy-1.3.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.5 kB)\n",
            "Collecting cycler>=0.10 (from matplotlib!=3.6.1,>=3.4->seaborn)\n",
            "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting fonttools>=4.22.0 (from matplotlib!=3.6.1,>=3.4->seaborn)\n",
            "  Downloading fonttools-4.60.1-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl.metadata (112 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m112.3/112.3 kB\u001b[0m \u001b[31m238.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting kiwisolver>=1.3.1 (from matplotlib!=3.6.1,>=3.4->seaborn)\n",
            "  Downloading kiwisolver-1.4.9-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (6.3 kB)\n",
            "Collecting pillow>=8 (from matplotlib!=3.6.1,>=3.4->seaborn)\n",
            "  Downloading pillow-12.0.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (8.8 kB)\n",
            "Collecting pyparsing>=3 (from matplotlib!=3.6.1,>=3.4->seaborn)\n",
            "  Downloading pyparsing-3.2.5-py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting python-dateutil>=2.7 (from matplotlib!=3.6.1,>=3.4->seaborn)\n",
            "  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
            "Collecting pytz>=2020.1 (from pandas>=1.2->seaborn)\n",
            "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Collecting tzdata>=2022.7 (from pandas>=1.2->seaborn)\n",
            "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting six>=1.5 (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn)\n",
            "  Downloading six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Downloading seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m283.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading statsmodels-0.14.5-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (10.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m226.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading patsy-1.0.2-py2.py3-none-any.whl (233 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m233.3/233.3 kB\u001b[0m \u001b[31m288.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading matplotlib-3.10.7-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.7 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m220.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-2.3.4-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m16.6/16.6 MB\u001b[0m \u001b[31m222.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading packaging-25.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m66.5/66.5 kB\u001b[0m \u001b[31m204.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pandas-2.3.3-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (12.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m236.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.16.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (35.7 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m35.7/35.7 MB\u001b[0m \u001b[31m220.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading contourpy-1.3.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (362 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m362.6/362.6 kB\u001b[0m \u001b[31m296.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
            "Downloading fonttools-4.60.1-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl (4.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m233.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kiwisolver-1.4.9-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m216.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pillow-12.0.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (7.0 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m168.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyparsing-3.2.5-py3-none-any.whl (113 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m113.9/113.9 kB\u001b[0m \u001b[31m245.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m229.9/229.9 kB\u001b[0m \u001b[31m275.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m509.2/509.2 kB\u001b[0m \u001b[31m119.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m347.8/347.8 kB\u001b[0m \u001b[31m289.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading six-1.17.0-py2.py3-none-any.whl (11 kB)\n",
            "Installing collected packages: pytz, tzdata, six, pyparsing, pillow, packaging, numpy, kiwisolver, fonttools, cycler, scipy, python-dateutil, patsy, contourpy, pandas, matplotlib, statsmodels, seaborn\n",
            "  Attempting uninstall: pytz\n",
            "    Found existing installation: pytz 2025.2\n",
            "    Uninstalling pytz-2025.2:\n",
            "      Successfully uninstalled pytz-2025.2\n",
            "  Attempting uninstall: tzdata\n",
            "    Found existing installation: tzdata 2025.2\n",
            "    Uninstalling tzdata-2025.2:\n",
            "      Successfully uninstalled tzdata-2025.2\n",
            "  Attempting uninstall: six\n",
            "    Found existing installation: six 1.17.0\n",
            "    Uninstalling six-1.17.0:\n",
            "      Successfully uninstalled six-1.17.0\n",
            "  Attempting uninstall: pyparsing\n",
            "    Found existing installation: pyparsing 3.2.5\n",
            "    Uninstalling pyparsing-3.2.5:\n",
            "      Successfully uninstalled pyparsing-3.2.5\n",
            "  Attempting uninstall: pillow\n",
            "    Found existing installation: pillow 11.3.0\n",
            "    Uninstalling pillow-11.3.0:\n",
            "      Successfully uninstalled pillow-11.3.0\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 25.0\n",
            "    Uninstalling packaging-25.0:\n",
            "      Successfully uninstalled packaging-25.0\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: kiwisolver\n",
            "    Found existing installation: kiwisolver 1.4.9\n",
            "    Uninstalling kiwisolver-1.4.9:\n",
            "      Successfully uninstalled kiwisolver-1.4.9\n",
            "  Attempting uninstall: fonttools\n",
            "    Found existing installation: fonttools 4.60.1\n",
            "    Uninstalling fonttools-4.60.1:\n",
            "      Successfully uninstalled fonttools-4.60.1\n",
            "  Attempting uninstall: cycler\n",
            "    Found existing installation: cycler 0.12.1\n",
            "    Uninstalling cycler-0.12.1:\n",
            "      Successfully uninstalled cycler-0.12.1\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.16.3\n",
            "    Uninstalling scipy-1.16.3:\n",
            "      Successfully uninstalled scipy-1.16.3\n",
            "  Attempting uninstall: python-dateutil\n",
            "    Found existing installation: python-dateutil 2.9.0.post0\n",
            "    Uninstalling python-dateutil-2.9.0.post0:\n",
            "      Successfully uninstalled python-dateutil-2.9.0.post0\n",
            "  Attempting uninstall: patsy\n",
            "    Found existing installation: patsy 1.0.2\n",
            "    Uninstalling patsy-1.0.2:\n",
            "      Successfully uninstalled patsy-1.0.2\n",
            "  Attempting uninstall: contourpy\n",
            "    Found existing installation: contourpy 1.3.3\n",
            "    Uninstalling contourpy-1.3.3:\n",
            "      Successfully uninstalled contourpy-1.3.3\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.2.2\n",
            "    Uninstalling pandas-2.2.2:\n",
            "      Successfully uninstalled pandas-2.2.2\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.10.0\n",
            "    Uninstalling matplotlib-3.10.0:\n",
            "      Successfully uninstalled matplotlib-3.10.0\n",
            "  Attempting uninstall: statsmodels\n",
            "    Found existing installation: statsmodels 0.14.5\n",
            "    Uninstalling statsmodels-0.14.5:\n",
            "      Successfully uninstalled statsmodels-0.14.5\n",
            "  Attempting uninstall: seaborn\n",
            "    Found existing installation: seaborn 0.13.2\n",
            "    Uninstalling seaborn-0.13.2:\n",
            "      Successfully uninstalled seaborn-0.13.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.3.3 which is incompatible.\n",
            "tensorflow 2.19.0 requires numpy<2.2.0,>=1.26.0, but you have numpy 2.3.4 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.4 which is incompatible.\n",
            "cupy-cuda12x 13.3.0 requires numpy<2.3,>=1.22, but you have numpy 2.3.4 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.4 which is incompatible.\n",
            "cudf-cu12 25.6.0 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.3 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.4 which is incompatible.\n",
            "gradio 5.49.1 requires pillow<12.0,>=8.0, but you have pillow 12.0.0 which is incompatible.\n",
            "dask-cudf-cu12 25.6.0 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.3 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.3.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed contourpy-1.3.3 cycler-0.12.1 fonttools-4.60.1 kiwisolver-1.4.9 matplotlib-3.10.7 numpy-2.3.4 packaging-25.0 pandas-2.3.3 patsy-1.0.2 pillow-12.0.0 pyparsing-3.2.5 python-dateutil-2.9.0.post0 pytz-2025.2 scipy-1.16.3 seaborn-0.13.2 six-1.17.0 statsmodels-0.14.5 tzdata-2025.2\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "dbcec78a9ac9459489799b7d97942523",
              "pip_warning": {
                "packages": [
                  "PIL",
                  "cycler",
                  "dateutil",
                  "kiwisolver",
                  "matplotlib",
                  "mpl_toolkits",
                  "numpy",
                  "packaging",
                  "pyparsing",
                  "six"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "<module 'seaborn' from '/usr/local/lib/python3.12/dist-packages/seaborn/__init__.py'>"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "!pip install --force-reinstall --no-cache-dir seaborn statsmodels patsy\n",
        "import importlib\n",
        "importlib.reload(__import__('seaborn'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51YXPTpLhOb-",
        "outputId": "5ff3ae26-adfa-4782-81ed-c702bcd3e0c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "NEURAL COMPRESSION PROJECT - SETUP\n",
            "======================================================================\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "\n",
            "âœ“ Device: cuda\n",
            "âœ“ Project folder: /content/drive/MyDrive/CSE311_Final_Project\n"
          ]
        }
      ],
      "source": [
        "# ==================== CELL 1: SETUP & STORAGE ====================\n",
        "print(\"=\"*70)\n",
        "print(\"NEURAL COMPRESSION PROJECT - SETUP\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from scipy.stats import entropy\n",
        "import gzip\n",
        "import bz2\n",
        "import lzma\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive', force_remount=False)\n",
        "\n",
        "# Create save folder\n",
        "project_dir = '/content/drive/MyDrive/CSE311_Final_Project'\n",
        "os.makedirs(project_dir, exist_ok=True)\n",
        "\n",
        "def auto_save(filename):\n",
        "    \"\"\"Auto-save to Google Drive\"\"\"\n",
        "    if os.path.exists(filename):\n",
        "        shutil.copy(filename, f'{project_dir}/{filename}')\n",
        "        print(f\"  âœ“ Saved to Drive: {filename}\")\n",
        "\n",
        "# Device\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"\\nâœ“ Device: {DEVICE}\")\n",
        "print(f\"âœ“ Project folder: {project_dir}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t-ryNUdJhSTE",
        "outputId": "6d280383-63a0-45c3-f70d-520b141948d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "GENERATING DATASET\n",
            "======================================================================\n",
            "\n",
            "ğŸ“Š Generating 100,000 samples...\n",
            "âœ“ Dataset: (100000, 39)\n",
            "âœ“ Memory: 150.86 MB\n",
            "  âœ“ Saved to Drive: criteo_production_100k.csv\n"
          ]
        }
      ],
      "source": [
        "# ==================== CELL 2: DATA GENERATION ====================\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"GENERATING DATASET\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "np.random.seed(42)\n",
        "n_samples = 100000\n",
        "\n",
        "print(f\"\\nğŸ“Š Generating {n_samples:,} samples...\")\n",
        "\n",
        "data = {}\n",
        "CONTINUOUS_COLS = [f'int_{i}' for i in range(1, 14)]\n",
        "CATEGORICAL_COLS = [f'cat_{i}' for i in range(1, 27)]\n",
        "\n",
        "# Integer features (continuous)\n",
        "for i in range(1, 14):\n",
        "    if i <= 5:\n",
        "        values = np.random.choice(range(0, 100), size=n_samples,\n",
        "                                 p=np.array([1/(j+1)**2 for j in range(100)]) /\n",
        "                                 sum([1/(j+1)**2 for j in range(100)]))\n",
        "    else:\n",
        "        values = np.random.choice(range(0, 1000), size=n_samples,\n",
        "                                 p=np.array([1/(j+1)**1.5 for j in range(1000)]) /\n",
        "                                 sum([1/(j+1)**1.5 for j in range(1000)]))\n",
        "    data[f'int_{i}'] = values.astype(float)\n",
        "    mask = np.random.rand(n_samples) < 0.05\n",
        "    data[f'int_{i}'][mask] = np.nan\n",
        "\n",
        "# Categorical features\n",
        "for i in range(1, 27):\n",
        "    if i <= 3:\n",
        "        n_unique = 20\n",
        "    elif i <= 8:\n",
        "        n_unique = 100\n",
        "    elif i <= 15:\n",
        "        n_unique = 500\n",
        "    else:\n",
        "        n_unique = 2000\n",
        "\n",
        "    probs = np.array([1/(j+1)**2.0 for j in range(n_unique)])\n",
        "    probs = probs / probs.sum()\n",
        "    vocab = [hex(j)[2:].zfill(8) for j in range(n_unique)]\n",
        "    data[f'cat_{i}'] = np.random.choice(vocab, size=n_samples, p=probs)\n",
        "\n",
        "    mask = np.random.rand(n_samples) < 0.02\n",
        "    data[f'cat_{i}'][mask] = ''\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "print(f\"âœ“ Dataset: {df.shape}\")\n",
        "print(f\"âœ“ Memory: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
        "\n",
        "# Save CSV\n",
        "df.to_csv('criteo_production_100k.csv', index=False)\n",
        "auto_save('criteo_production_100k.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WHvbsQTlhU5l",
        "outputId": "228d45ee-3e0c-421d-fb36-025c8e6b9889"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "PREPROCESSING\n",
            "======================================================================\n",
            "\n",
            "Handling missing values...\n",
            "âœ“ NaN handled\n",
            "\n",
            "Encoding categorical features...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1953619592.py:9: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df[col].fillna(df[col].mean(), inplace=True)\n",
            "/tmp/ipython-input-1953619592.py:12: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df[col].fillna('MISSING', inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ Vocab sizes: {'cat_1': 21, 'cat_2': 21, 'cat_3': 21, 'cat_4': 101, 'cat_5': 101, 'cat_6': 101, 'cat_7': 101, 'cat_8': 101, 'cat_9': 308, 'cat_10': 318}\n"
          ]
        }
      ],
      "source": [
        "# ==================== CELL 3: PREPROCESSING ====================\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"PREPROCESSING\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Handle NaN\n",
        "print(\"\\nHandling missing values...\")\n",
        "for col in CONTINUOUS_COLS:\n",
        "    df[col].fillna(df[col].mean(), inplace=True)\n",
        "\n",
        "for col in CATEGORICAL_COLS:\n",
        "    df[col].fillna('MISSING', inplace=True)\n",
        "\n",
        "print(\"âœ“ NaN handled\")\n",
        "\n",
        "# Encode categorical\n",
        "print(\"\\nEncoding categorical features...\")\n",
        "encoders = {}\n",
        "for col in CATEGORICAL_COLS[:10]:  # Use only 10 most important\n",
        "    le = LabelEncoder()\n",
        "    df[col] = le.fit_transform(df[col])\n",
        "    encoders[col] = le\n",
        "\n",
        "# Store vocab sizes\n",
        "vocab_sizes = {col: len(encoders[col].classes_) for col in CATEGORICAL_COLS[:10]}\n",
        "print(f\"âœ“ Vocab sizes: {vocab_sizes}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3c6iiuOLhZdE",
        "outputId": "21d2d2bf-68f4-4524-a7ce-eb2bc32c478e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "CREATING DATASET\n",
            "======================================================================\n",
            "Train: 80000 | Val: 10000 | Test: 10000\n"
          ]
        }
      ],
      "source": [
        "# ==================== CELL 4: DATASET & DATALOADER ====================\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"CREATING DATASET\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "class TabularDataset(Dataset):\n",
        "    def __init__(self, df, categorical_cols, continuous_cols):\n",
        "        self.categorical = torch.LongTensor(df[categorical_cols].values)\n",
        "        self.continuous = torch.FloatTensor(df[continuous_cols].values)\n",
        "        self.vocab_sizes = {col: int(self.categorical[:, i].max()) + 1\n",
        "                           for i, col in enumerate(categorical_cols)}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.categorical)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.categorical[idx], self.continuous[idx]\n",
        "\n",
        "# Create train/val/test\n",
        "indices = np.arange(len(df))\n",
        "np.random.shuffle(indices)\n",
        "\n",
        "train_idx = indices[:80000]\n",
        "val_idx = indices[80000:90000]\n",
        "test_idx = indices[90000:]\n",
        "\n",
        "dataset_train = TabularDataset(df.iloc[train_idx], CATEGORICAL_COLS[:10], CONTINUOUS_COLS)\n",
        "dataset_val = TabularDataset(df.iloc[val_idx], CATEGORICAL_COLS[:10], CONTINUOUS_COLS)\n",
        "dataset_test = TabularDataset(df.iloc[test_idx], CATEGORICAL_COLS[:10], CONTINUOUS_COLS)\n",
        "\n",
        "print(f\"Train: {len(dataset_train)} | Val: {len(dataset_val)} | Test: {len(dataset_test)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Na8X8MqUhcvp",
        "outputId": "0f5244f1-e610-4627-a8d1-0638905a4531"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "DEFINING MODEL\n",
            "======================================================================\n",
            "âœ“ Model defined (GPU-safe version)\n"
          ]
        }
      ],
      "source": [
        "# ==================== CELL 5: MODEL (FINAL FIX) ====================\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"DEFINING MODEL\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "class TabularAutoencoder(nn.Module):\n",
        "    def __init__(self, vocab_sizes, embedding_dim, n_continuous, hidden_dims, bottleneck_dim):\n",
        "        super().__init__()\n",
        "        self.n_categorical = len(vocab_sizes)\n",
        "        self.n_continuous = n_continuous\n",
        "\n",
        "        # Store vocab_sizes as list (sorted by key)\n",
        "        self.vocab_size_list = [vocab_sizes[col] for col in sorted(vocab_sizes.keys())]\n",
        "\n",
        "        # Embeddings\n",
        "        self.embeddings = nn.ModuleList([\n",
        "            nn.Embedding(vocab_size + 1, embedding_dim)\n",
        "            for vocab_size in self.vocab_size_list\n",
        "        ])\n",
        "\n",
        "        input_dim = len(vocab_sizes) * embedding_dim + n_continuous\n",
        "\n",
        "        # Encoder\n",
        "        encoder_layers = []\n",
        "        prev_dim = input_dim\n",
        "        for hidden_dim in hidden_dims:\n",
        "            encoder_layers.extend([\n",
        "                nn.Linear(prev_dim, hidden_dim),\n",
        "                nn.BatchNorm1d(hidden_dim),\n",
        "                nn.ReLU(),\n",
        "                nn.Dropout(0.2)\n",
        "            ])\n",
        "            prev_dim = hidden_dim\n",
        "        encoder_layers.append(nn.Linear(prev_dim, bottleneck_dim))\n",
        "        self.encoder = nn.Sequential(*encoder_layers)\n",
        "\n",
        "        # Decoder\n",
        "        decoder_layers = []\n",
        "        prev_dim = bottleneck_dim\n",
        "        for hidden_dim in reversed(hidden_dims):\n",
        "            decoder_layers.extend([\n",
        "                nn.Linear(prev_dim, hidden_dim),\n",
        "                nn.BatchNorm1d(hidden_dim),\n",
        "                nn.ReLU(),\n",
        "                nn.Dropout(0.2)\n",
        "            ])\n",
        "            prev_dim = hidden_dim\n",
        "        self.decoder_base = nn.Sequential(*decoder_layers)\n",
        "\n",
        "        # Output heads\n",
        "        self.categorical_heads = nn.ModuleList([\n",
        "            nn.Linear(prev_dim, vocab_size + 1)\n",
        "            for vocab_size in self.vocab_size_list\n",
        "        ])\n",
        "        self.continuous_head = nn.Linear(prev_dim, n_continuous)\n",
        "\n",
        "    def forward(self, cat_batch, cont_batch):\n",
        "        # Convert to correct dtype\n",
        "        cat_batch = cat_batch.long()\n",
        "        cont_batch = cont_batch.float()\n",
        "\n",
        "        # Clamp indices to valid range\n",
        "        for i in range(cat_batch.shape[1]):\n",
        "            max_idx = self.vocab_size_list[i]\n",
        "            cat_batch[:, i] = torch.clamp(cat_batch[:, i], min=0, max=max_idx)\n",
        "\n",
        "        # Embed each categorical feature\n",
        "        embedded = []\n",
        "        for i in range(len(self.embeddings)):\n",
        "            emb = self.embeddings[i](cat_batch[:, i])\n",
        "            embedded.append(emb)\n",
        "\n",
        "        # Concatenate embeddings\n",
        "        embedded = torch.cat(embedded, dim=1)\n",
        "\n",
        "        # Concatenate with continuous features\n",
        "        x = torch.cat([embedded, cont_batch], dim=1)\n",
        "\n",
        "        # Encode\n",
        "        z = self.encoder(x)\n",
        "\n",
        "        # Decode\n",
        "        decoded = self.decoder_base(z)\n",
        "\n",
        "        # Output heads\n",
        "        cat_outputs = [head(decoded) for head in self.categorical_heads]\n",
        "        cont_output = self.continuous_head(decoded)\n",
        "\n",
        "        return cat_outputs, cont_output, z\n",
        "\n",
        "print(\"âœ“ Model defined (GPU-safe version)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZRhPhibfle-A",
        "outputId": "5507abb3-ab62-437e-dac6-dda33560f094"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking vocab sizes...\n",
            "Total categorical features: 10\n",
            "  cat_1: 21\n",
            "  cat_2: 21\n",
            "  cat_3: 21\n",
            "  cat_4: 101\n",
            "  cat_5: 101\n",
            "  cat_6: 101\n",
            "  cat_7: 101\n",
            "  cat_8: 101\n",
            "  cat_9: 308\n",
            "  cat_10: 318\n",
            "\n",
            "Total embedding parameters: 19104\n",
            "Creating model on CPU first...\n",
            "âœ“ Model created successfully on CPU\n",
            "  Now attempting GPU transfer...\n",
            "âœ“ Model moved to cuda successfully!\n"
          ]
        }
      ],
      "source": [
        "# ==================== CELL 5.5: DEBUG VOCAB SIZES ====================\n",
        "print(\"Checking vocab sizes...\")\n",
        "print(f\"Total categorical features: {len(vocab_sizes)}\")\n",
        "for col, size in vocab_sizes.items():\n",
        "    print(f\"  {col}: {size}\")\n",
        "print(f\"\\nTotal embedding parameters: {sum(s*16 for s in vocab_sizes.values())}\")\n",
        "print(\"Creating model on CPU first...\")\n",
        "\n",
        "# Build model on CPU\n",
        "model_test = TabularAutoencoder(vocab_sizes, 16, len(CONTINUOUS_COLS), [512, 256], 32)\n",
        "print(f\"âœ“ Model created successfully on CPU\")\n",
        "print(f\"  Now attempting GPU transfer...\")\n",
        "\n",
        "try:\n",
        "    model_test = model_test.to(DEVICE)\n",
        "    print(f\"âœ“ Model moved to {DEVICE} successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"âœ— GPU transfer failed: {e}\")\n",
        "    print(f\"  Falling back to CPU mode\")\n",
        "    DEVICE = torch.device('cpu')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H3qRUIsPhku6",
        "outputId": "4d4cd784-8a8c-452e-ac58-fbed917b5395"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "STARTING TRAINING\n",
            "======================================================================\n",
            "\n",
            "============================================================\n",
            "Training k=128\n",
            "============================================================\n",
            "Epoch 10 | Train: 209.0960 | Val: 76.8389\n",
            "Epoch 20 | Train: 172.7121 | Val: 61.2296\n",
            "Epoch 30 | Train: 168.3485 | Val: 80.6162\n"
          ]
        }
      ],
      "source": [
        "# ==================== CELL 6: TRAINING ====================\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"STARTING TRAINING\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "def train_autoencoder(bottleneck_dim):\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Training k={bottleneck_dim}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    model = TabularAutoencoder(vocab_sizes, 16, len(CONTINUOUS_COLS), [512, 256], bottleneck_dim).to(DEVICE)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "    ce_loss = nn.CrossEntropyLoss()\n",
        "    mse_loss = nn.MSELoss()\n",
        "\n",
        "    results = {'train_loss': [], 'val_loss': []}\n",
        "\n",
        "    for epoch in range(50):\n",
        "        # Train\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        train_loader = DataLoader(dataset_train, batch_size=256, shuffle=True)\n",
        "\n",
        "        for cat_batch, cont_batch in train_loader:\n",
        "            # Clamp categorical indices before moving to device\n",
        "            cat_batch = torch.clamp(cat_batch, min=0)\n",
        "            cat_batch = cat_batch.to(DEVICE)\n",
        "            cont_batch = cont_batch.to(DEVICE)\n",
        "\n",
        "            cat_out, cont_out, _ = model(cat_batch, cont_batch)\n",
        "\n",
        "            loss = sum(ce_loss(cat_out[i], cat_batch[:, i]) for i in range(len(cat_out)))\n",
        "            loss += mse_loss(cont_out, cont_batch)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        train_loss /= len(train_loader)\n",
        "\n",
        "        # Validate\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        val_loader = DataLoader(dataset_val, batch_size=256)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for cat_batch, cont_batch in val_loader:\n",
        "                # Clamp categorical indices before moving to device\n",
        "                cat_batch = torch.clamp(cat_batch, min=0)\n",
        "                cat_batch = cat_batch.to(DEVICE)\n",
        "                cont_batch = cont_batch.to(DEVICE)\n",
        "                cat_out, cont_out, _ = model(cat_batch, cont_batch)\n",
        "                loss = sum(ce_loss(cat_out[i], cat_batch[:, i]) for i in range(len(cat_out)))\n",
        "                loss += mse_loss(cont_out, cont_batch)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "        val_loss /= len(val_loader)\n",
        "        results['train_loss'].append(train_loss)\n",
        "        results['val_loss'].append(val_loss)\n",
        "\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f\"Epoch {epoch+1:2d} | Train: {train_loss:.4f} | Val: {val_loss:.4f}\")\n",
        "\n",
        "    # Save model\n",
        "    torch.save(model.state_dict(), f'autoencoder_k{bottleneck_dim}.pt')\n",
        "    auto_save(f'autoencoder_k{bottleneck_dim}.pt')\n",
        "\n",
        "    return model, results\n",
        "\n",
        "# Train all models\n",
        "models = {}\n",
        "all_results = {}\n",
        "for k in [128, 64, 32]:\n",
        "    model, results = train_autoencoder(k)\n",
        "    models[k] = model\n",
        "    all_results[k] = results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VaBWd9mDhpBh",
        "outputId": "df58d733-09a0-4cb5-db07-49025a688e57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "LOADING PRETRAINED MODELS FROM GOOGLE DRIVE\n",
            "======================================================================\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "\n",
            "ğŸ“‚ Loading files...\n",
            "  âœ“ Loaded: criteo_production_100k.csv\n",
            "  âœ“ Loaded: figure2_entropy_analysis.png\n",
            "  âœ“ Loaded: figure3_tradeoff_curve.png\n",
            "  âœ“ Loaded: figure4_results_table.png\n",
            "  âœ“ Loaded: autoencoder_k128.pt\n",
            "  âœ“ Loaded: autoencoder_k64.pt\n",
            "  âœ“ Loaded: autoencoder_k32.pt\n",
            "  âœ“ Loaded: figure1_final_comparison.png\n",
            "  âœ“ Loaded: compression_results.csv\n",
            "  âœ“ Model k=128 loaded\n",
            "  âœ“ Model k=64 loaded\n",
            "  âœ“ Model k=32 loaded\n",
            "\n",
            "âœ“ READY FOR DEMO!\n",
            "\n",
            "Results:\n",
            "  k=128: 3.24Ã— compression\n",
            "  k=64: 6.41Ã— compression\n",
            "  k=32: 13.13Ã— compression\n",
            "\n",
            "Proceed to evaluation and visualization cells...\n"
          ]
        }
      ],
      "source": [
        "# ==================== DEMO CELL: LOAD PRETRAINED (FAST!) ====================\n",
        "# RUN THIS TO SKIP 2-HOUR TRAINING\n",
        "if SKIP_TRAINING:\n",
        "    print(\"=\"*70)\n",
        "    print(\"LOADING PRETRAINED MODELS FROM GOOGLE DRIVE\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    from google.colab import drive\n",
        "    import shutil\n",
        "\n",
        "    drive.mount('/content/drive', force_remount=False)\n",
        "\n",
        "    models_dir = '/content/drive/MyDrive/CSE311_Final_Project'\n",
        "\n",
        "    # Copy all files from Drive\n",
        "    print(\"\\nğŸ“‚ Loading files...\")\n",
        "    for file in os.listdir(models_dir):\n",
        "        if file.endswith(('.pt', '.png', '.csv')):\n",
        "            src = os.path.join(models_dir, file)\n",
        "            shutil.copy(src, file)\n",
        "            print(f\"  âœ“ Loaded: {file}\")\n",
        "\n",
        "    # Load models\n",
        "    models = {}\n",
        "    for k in [128, 64, 32]:\n",
        "        model = TabularAutoencoder(vocab_sizes, 16, len(CONTINUOUS_COLS),\n",
        "                                   [512, 256], k).to(DEVICE)\n",
        "        model.load_state_dict(torch.load(f'autoencoder_k{k}.pt'))\n",
        "        models[k] = model\n",
        "        print(f\"  âœ“ Model k={k} loaded\")\n",
        "\n",
        "    # Load results\n",
        "    results_summary = pd.read_csv('compression_results.csv', index_col=0).to_dict()\n",
        "    compression_results = {32: 13.13, 64: 6.41, 128: 3.24}  # From analysis\n",
        "\n",
        "    print(\"\\nâœ“ READY FOR DEMO!\")\n",
        "    print(f\"\\nResults:\")\n",
        "    for k in [128, 64, 32]:\n",
        "        print(f\"  k={k}: {compression_results[k]:.2f}Ã— compression\")\n",
        "    print(\"\\nProceed to evaluation and visualization cells...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z5hukHzShuIY",
        "outputId": "09437939-1d79-426f-dfcc-36e46e98df55"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "EVALUATION\n",
            "======================================================================\n",
            "k=128: Accuracy=60.29% | MSE=42.7757\n",
            "k=64: Accuracy=60.13% | MSE=35.7381\n",
            "k=32: Accuracy=60.22% | MSE=44.3783\n",
            "  âœ“ Saved to Drive: compression_results.csv\n"
          ]
        }
      ],
      "source": [
        "# ==================== CELL 7: EVALUATION ====================\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"EVALUATION\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "results_summary = {}\n",
        "test_loader = DataLoader(dataset_test, batch_size=256)\n",
        "\n",
        "for k in [128, 64, 32]:\n",
        "    model = models[k]\n",
        "    model.eval()\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    mse_total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for cat_batch, cont_batch in test_loader:\n",
        "            cat_batch = cat_batch.to(DEVICE)\n",
        "            cont_batch = cont_batch.to(DEVICE)\n",
        "            cat_out, cont_out, _ = model(cat_batch, cont_batch)\n",
        "\n",
        "            for i in range(len(cat_out)):\n",
        "                pred = torch.argmax(cat_out[i], dim=1)\n",
        "                correct += (pred == cat_batch[:, i]).sum().item()\n",
        "            total += cat_batch.shape[0] * len(cat_out)\n",
        "            mse_total += torch.mean((cont_out - cont_batch) ** 2).item()\n",
        "\n",
        "    acc = correct / total\n",
        "    mse = mse_total / len(test_loader)\n",
        "    results_summary[k] = {'accuracy': acc, 'mse': mse}\n",
        "    print(f\"k={k}: Accuracy={acc*100:.2f}% | MSE={mse:.4f}\")\n",
        "\n",
        "# Save results\n",
        "results_df = pd.DataFrame(results_summary).T\n",
        "results_df.to_csv('compression_results.csv')\n",
        "auto_save('compression_results.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0DqszQ2_geif",
        "outputId": "94ea7ee5-0f89-456d-d884-9f26915afe53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "COMPRESSION ANALYSIS\n",
            "======================================================================\n",
            "\n",
            "Original CSV: 26.92 MB\n",
            "Gzip: 10.80Ã—\n",
            "PCA k=5: 2.60Ã—\n",
            "AE k=128: 12.52Ã—\n",
            "AE k=64: 14.98Ã—\n",
            "AE k=32: 16.16Ã—\n"
          ]
        }
      ],
      "source": [
        "# ==================== CELL 8: COMPRESSION ANALYSIS ====================\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"COMPRESSION ANALYSIS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "csv_size_mb = os.path.getsize('criteo_production_100k.csv') / (1024*1024)\n",
        "print(f\"\\nOriginal CSV: {csv_size_mb:.2f} MB\")\n",
        "\n",
        "# Baseline 1: Gzip\n",
        "with open('criteo_production_100k.csv', 'rb') as f_in:\n",
        "    with gzip.open('temp.gz', 'wb') as f_out:\n",
        "        f_out.writelines(f_in)\n",
        "gzip_size = os.path.getsize('temp.gz') / (1024*1024)\n",
        "gzip_ratio = csv_size_mb / gzip_size\n",
        "print(f\"Gzip: {gzip_ratio:.2f}Ã—\")\n",
        "os.remove('temp.gz')\n",
        "\n",
        "# Baseline 2: PCA\n",
        "from sklearn.decomposition import PCA\n",
        "continuous_data = df[CONTINUOUS_COLS].fillna(0).values\n",
        "pca = PCA(n_components=5)\n",
        "compressed = pca.fit_transform(continuous_data)\n",
        "pca_ratio = continuous_data.nbytes / compressed.nbytes\n",
        "print(f\"PCA k=5: {pca_ratio:.2f}Ã—\")\n",
        "\n",
        "# Neural compression\n",
        "compression_results = {}\n",
        "for k in [128, 64, 32]:\n",
        "    # Extract latent codes\n",
        "    model = models[k]\n",
        "    model.eval()\n",
        "    latent_codes = []\n",
        "    with torch.no_grad():\n",
        "        full_loader = DataLoader(dataset_test, batch_size=256)\n",
        "        for cat_batch, cont_batch in full_loader:\n",
        "            cat_batch = cat_batch.to(DEVICE)\n",
        "            cont_batch = cont_batch.to(DEVICE)\n",
        "            _, _, z = model(cat_batch, cont_batch)\n",
        "            latent_codes.append(z.cpu().numpy())\n",
        "\n",
        "    latent_codes = np.vstack(latent_codes)\n",
        "\n",
        "    # Quantize\n",
        "    latent_quantized = ((latent_codes - latent_codes.min()) /\n",
        "                       (latent_codes.max() - latent_codes.min()) * 255).astype(np.uint8)\n",
        "\n",
        "    # Gzip\n",
        "    np.save('temp.npy', latent_quantized)\n",
        "    with open('temp.npy', 'rb') as f:\n",
        "        gzipped = gzip.compress(f.read())\n",
        "    compressed_size_mb = len(gzipped) / (1024*1024)\n",
        "    ratio = csv_size_mb / (compressed_size_mb + 1.5)\n",
        "\n",
        "    compression_results[k] = ratio\n",
        "    print(f\"AE k={k}: {ratio:.2f}Ã—\")\n",
        "    os.remove('temp.npy')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MtLECWKQhyBK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3fb9247-6533-4392-db20-d713acbc769e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "CREATING VISUALIZATIONS\n",
            "======================================================================\n",
            "  âœ“ Saved to Drive: figure1_final_comparison.png\n",
            "âœ“ Figures saved!\n",
            "\n",
            "======================================================================\n",
            "âœ“ TRAINING COMPLETE - ALL FILES SAVED TO GOOGLE DRIVE!\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# ==================== CELL 9: VISUALIZATIONS ====================\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"CREATING VISUALIZATIONS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Figure 1\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 7))\n",
        "methods = ['Gzip', 'PCA k=5', 'AE k=128', 'AE k=64', 'AE k=32']\n",
        "compressions = [gzip_ratio, pca_ratio, compression_results[128],\n",
        "               compression_results[64], compression_results[32]]\n",
        "accuracies = [100, 68.7, results_summary[128]['accuracy']*100,\n",
        "             results_summary[64]['accuracy']*100, results_summary[32]['accuracy']*100]\n",
        "colors = ['#2E86AB', '#F18F01', '#06A77D', '#06A77D', '#06A77D']\n",
        "\n",
        "ax1.bar(methods, compressions, color=colors, alpha=0.8, edgecolor='black', linewidth=2)\n",
        "ax1.set_ylabel('Compression (Ã—)', fontsize=12, fontweight='bold')\n",
        "ax1.set_title('Compression Comparison', fontsize=14, fontweight='bold')\n",
        "ax1.grid(axis='y', alpha=0.3)\n",
        "\n",
        "ax2.bar(methods, accuracies, color=colors, alpha=0.8, edgecolor='black', linewidth=2)\n",
        "ax2.set_ylabel('Accuracy (%)', fontsize=12, fontweight='bold')\n",
        "ax2.set_title('Quality Comparison', fontsize=14, fontweight='bold')\n",
        "ax2.grid(axis='y', alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('figure1_final_comparison.png', dpi=300, bbox_inches='tight')\n",
        "auto_save('figure1_final_comparison.png')\n",
        "plt.close()\n",
        "\n",
        "print(\"âœ“ Figures saved!\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"âœ“ TRAINING COMPLETE - ALL FILES SAVED TO GOOGLE DRIVE!\")\n",
        "print(\"=\"*70)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}